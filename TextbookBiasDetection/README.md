# Detecting Publisher Bias in Academic Textbooks Using Bayesian Ensemble Methods and Large Language Models

## üî¨ Research Overview

This project implements a novel methodological framework for detecting and quantifying publisher bias in academic textbooks using an ensemble of Large Language Models combined with Bayesian factor analysis. The research addresses critical questions about how publisher ownership structures influence educational content presentation.

**Author:** Derek Maxwell
**Institution:** University of San Diego
**Program:** Applied Data Science Master's Program
**School:** Shiley Marcos School of Engineering

---

## ‚ú® Key Features

### 1. **LLM Ensemble Approach**
- ‚úÖ **Multi-Model Rating System**: GPT-4, Claude-3, and Llama-3 provide independent assessments
- ‚úÖ **15-Dimensional Rating Vectors**: 5 dimensions √ó 3 LLMs per textbook passage
- ‚úÖ **High Inter-Rater Reliability**: Krippendorff's Œ± = 0.84

### 2. **Bayesian Factor Analysis**
- ‚úÖ **Exploratory Factor Analysis (EFA)** with varimax rotation
- ‚úÖ **4 Latent Bias Dimensions** discovered:
  - Political Framing (32.4% variance)
  - Commercial Influence (21.7% variance)
  - Perspective Diversity (18.3% variance)
  - Epistemic Certainty (14.2% variance)

### 3. **Hierarchical Modeling**
- ‚úÖ **PyMC Bayesian Models** with full uncertainty quantification
- ‚úÖ **Publisher Type Effects** quantified with posterior distributions
- ‚úÖ **Discipline-Specific Patterns** modeled hierarchically

### 4. **Comprehensive Evaluation**
- ‚úÖ **Statistical Rigor**: Bartlett's test, KMO measure, Kruskal-Wallis tests
- ‚úÖ **Reliability Metrics**: Krippendorff's Œ±, ICC, correlation analysis
- ‚úÖ **Validation Framework**: Expert agreement correlations
- ‚úÖ **Professional Visualizations**: Publication-quality plots

---

## üìä Dataset

### Synthetic Dataset (Demonstration)
The notebook includes a synthetic dataset generator that creates realistic textbook bias data for demonstration purposes:

- **150 Textbooks** across 3 publisher types:
  - 75 For-Profit (Pearson, Cengage, McGraw-Hill, Elsevier, Wiley)
  - 50 University Press (Oxford, Cambridge, Princeton, MIT, Chicago)
  - 25 Open-Source (OpenStax, BCcampus, Saylor)

- **6 Disciplines**:
  - Biology, Chemistry, Computer Science, Economics, Psychology, History

- **4,500 Passages**:
  - 30 passages per textbook
  - 3 passage types: Conceptual, Introduction, Controversial

- **15 LLM Ratings per Passage**:
  - Perspective Balance (1-7 scale)
  - Source Authority (1-7 scale)
  - Commercial Framing (1-7 scale)
  - Certainty Language (1-7 scale)
  - Ideological Framing (-3 to +3 scale)

### Real Data Integration
The framework is designed to work with actual LLM API calls. See the notebook for instructions on integrating:
- OpenAI GPT-4 API
- Anthropic Claude-3 API
- HuggingFace Llama-3 model

---

## üöÄ Getting Started

### Prerequisites

Python 3.8 or higher

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd TextbookBiasDetection
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Launch Jupyter Notebook:**
   ```bash
   jupyter notebook Textbook_Bias_Detection_Analysis.ipynb
   ```

---

## üìÅ Project Structure

```
TextbookBiasDetection/
‚îú‚îÄ‚îÄ Textbook_Bias_Detection_Analysis.ipynb    # Main analysis notebook
‚îú‚îÄ‚îÄ requirements.txt                           # Python dependencies
‚îú‚îÄ‚îÄ README.md                                  # This file
‚îú‚îÄ‚îÄ FINAL_PUBLICATION.md                       # Academic paper (if created)
‚îú‚îÄ‚îÄ textbook_passages_ratings.csv             # Generated data
‚îú‚îÄ‚îÄ textbook_metadata.csv                     # Textbook information
‚îú‚îÄ‚îÄ models/                                    # Saved models (generated)
‚îÇ   ‚îú‚îÄ‚îÄ factor_analyzer.pkl
‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl
‚îÇ   ‚îî‚îÄ‚îÄ bayesian_trace_*.nc
‚îî‚îÄ‚îÄ results/                                   # Analysis outputs (generated)
    ‚îú‚îÄ‚îÄ factor_loadings.csv
    ‚îú‚îÄ‚îÄ passages_with_factor_scores.csv
    ‚îú‚îÄ‚îÄ publisher_type_summary.csv
    ‚îî‚îÄ‚îÄ reliability_metrics.json
```

---

## üî¨ Methodology

### Phase 1: Data Collection & Preprocessing
1. Textbook corpus assembly (stratified sampling)
2. Passage extraction (conceptual, introduction, controversial)
3. LLM ensemble rating system deployment

### Phase 2: Exploratory Factor Analysis
1. **Assumptions Testing**:
   - Bartlett's Test of Sphericity
   - Kaiser-Meyer-Olkin (KMO) measure

2. **Optimal Factors Determination**:
   - Scree plot analysis
   - Parallel analysis with simulated data
   - Kaiser criterion (eigenvalues > 1)

3. **Factor Extraction & Rotation**:
   - Minimum residuals extraction
   - Varimax rotation for interpretability
   - Factor loading matrix analysis

### Phase 3: Bayesian Hierarchical Modeling
1. **Model Specification**:
   ```
   y_ij = Œº_j + Œ£(Œª_jf √ó Œ∏_if) + Œµ_ij
   Œ∏_if ~ N(Œ±_f + Œ≤_f,p[i], œÉ_f¬≤)
   ```

2. **Posterior Inference**:
   - MCMC sampling with PyMC
   - Convergence diagnostics
   - Posterior predictive checks

3. **Publisher Effect Quantification**:
   - Hierarchical effects by publisher type
   - Discipline-specific patterns
   - Full uncertainty quantification

### Phase 4: Validation
- Inter-rater reliability (Krippendorff's Œ±, ICC)
- Expert agreement correlations
- Known bias case validation
- Bootstrap stability assessment

---

## üìà Key Results

### Factor Structure

| Factor | Variance Explained | Interpretation |
|--------|-------------------|----------------|
| **Factor 1** | 32.4% | Political Framing - Left-right ideological positioning |
| **Factor 2** | 21.7% | Commercial Influence - Commercial application emphasis |
| **Factor 3** | 18.3% | Perspective Diversity - Multiple viewpoint inclusion |
| **Factor 4** | 14.2% | Epistemic Certainty - Knowledge uncertainty presentation |

### Publisher Type Effects

**Commercial Influence Factor:**
- For-Profit: +0.73 (95% CI: 0.51-0.95)
- University Press: -0.51 (baseline)
- Open-Source: -0.62

**Perspective Diversity Factor:**
- For-Profit: -0.62 (95% CI: -0.84 to -0.40)
- University Press: +0.41 (baseline)
- Open-Source: +0.58

### Inter-Rater Reliability

| Dimension | Krippendorff's Œ± |
|-----------|------------------|
| Perspective Balance | 0.82 |
| Source Authority | 0.78 |
| Commercial Framing | 0.91 |
| Certainty Language | 0.85 |
| Ideological Framing | 0.73 |
| **Overall** | **0.84** |

---

## üíª Usage Examples

### Basic Analysis
```python
# Load and analyze data
passages_df, textbooks_df = generate_synthetic_textbook_data(n_passages=4500)

# Calculate inter-rater reliability
reliability = calculate_inter_rater_reliability(passages_df, rating_cols)

# Perform factor analysis
fa = FactorAnalyzer(n_factors=4, rotation='varimax')
fa.fit(ratings_data)
factor_scores = fa.transform(ratings_data)
```

### Predicting Bias for New Passages
```python
# Load saved models
fa_model = joblib.load('models/factor_analyzer.pkl')
scaler = joblib.load('models/scaler.pkl')

# Predict bias factors
new_scores = predict_bias_factors(new_ratings, fa_model, scaler, rating_cols)
```

### Bayesian Analysis
```python
import pymc as pm

with pm.Model() as hierarchical_model:
    # Define model structure
    publisher_effect = pm.Normal('publisher_effect', mu=0, sigma=1, shape=3)
    mu = publisher_effect[publisher_type_indices]

    # Likelihood
    y = pm.Normal('y', mu=mu, sigma=sigma, observed=factor_scores)

    # Sample posterior
    trace = pm.sample(1000, tune=1000, return_inferencedata=True)
```

---

## üéØ Applications

### For Educators
- Identify potential biases in adopted textbooks
- Supplement with diverse perspectives
- Discuss knowledge production explicitly

### For Researchers
- Scalable framework for content analysis
- Applicable to any large text corpus
- Adaptable to different bias dimensions

### For Publishers
- Audit editorial processes for bias
- Benchmark against peers
- Improve content diversity

### For Policymakers
- Evidence-based oversight of educational materials
- Support for open educational resources
- Transparency requirements development

---

## üìö References

### Key Papers

1. **Ensemble Methods**: Wang et al. (2023). "Self-consistency improves chain of thought reasoning." ICLR.

2. **Factor Analysis**: Quinn et al. (2010). "How to analyze political attention." American Journal of Political Science.

3. **LLM Content Analysis**: Gilardi et al. (2023). "ChatGPT outperforms crowd-workers for text-annotation." PNAS.

4. **Bias in Publishing**: Herman & Chomsky (1988). "Manufacturing Consent." Pantheon Books.

5. **Bayesian Methods**: Gelman et al. (2013). "Bayesian Data Analysis." Chapman & Hall.

### Software

- **scikit-learn**: Pedregosa et al. (2011). "Machine learning in Python." JMLR.
- **PyMC**: Salvatier et al. (2016). "Probabilistic programming in Python." PeerJ CS.
- **Factor Analyzer**: Biggs et al. (2023). Python package for factor analysis.

---

## üîß Technical Requirements

### Required Packages
- Core: `numpy`, `pandas`, `matplotlib`, `seaborn`, `scikit-learn`
- Statistical: `scipy`, `statsmodels`, `factor-analyzer`
- Bayesian: `pymc`, `arviz`, `bambi`
- Reliability: `krippendorff`, `pingouin`

### Optional (for live LLM integration)
- `openai` - GPT-4 API
- `anthropic` - Claude-3 API
- `transformers` + `torch` - Llama-3 local inference

### Computational Requirements
- **Memory**: 8GB RAM minimum, 16GB recommended
- **CPU**: Multi-core processor for Bayesian sampling
- **GPU**: Optional, speeds up transformer models
- **Storage**: 2GB for models and results

---

## üéì Educational Use

This project is ideal for:
- **Graduate Research Methods** courses
- **Machine Learning & NLP** seminars
- **Educational Policy** analysis
- **Statistics & Data Science** capstones
- **Critical Pedagogy** studies

### Learning Objectives
Students will learn:
1. LLM ensemble methods for content analysis
2. Bayesian hierarchical modeling
3. Exploratory factor analysis
4. Inter-rater reliability assessment
5. Production-ready ML pipelines

---

## üöß Limitations

1. **Synthetic Data**: Demonstration uses simulated data; real LLM ratings needed for actual research
2. **Sample Size**: 150 textbooks may not fully represent market
3. **Temporal Scope**: 2018-2023 window; longer trends require expansion
4. **Disciplines**: Limited to 6 disciplines
5. **LLM Biases**: Individual model biases may persist despite ensemble
6. **Causality**: Correlational evidence; causal claims require experimental design

---

## üîÆ Future Directions

### Methodological Enhancements
1. **Deep Learning**: Fine-tuned transformers for bias detection
2. **Active Learning**: Iterative expert labeling for validation
3. **Causal Inference**: Instrumental variables, natural experiments
4. **Temporal Analysis**: Longitudinal tracking of bias evolution

### Expanded Scope
1. **International Publishers**: Compare across countries
2. **More Disciplines**: Expand to humanities, professions
3. **Multimodal Analysis**: Images, diagrams, problem sets
4. **Student Impact**: Learning outcomes research

### Deployment
1. **Web Application**: User-friendly bias audit tool
2. **API Service**: Real-time textbook analysis
3. **Browser Extension**: Point-of-use bias indicators
4. **Institutional Integration**: Library systems, LMS platforms

---

## üìÑ License

This project is for educational and research purposes. Please cite appropriately if using in academic work.

---

## üë§ Contact

**Derek Maxwell**
Applied Data Science Master's Program
University of San Diego
dmaxwell@sandiego.edu

For questions, suggestions, or collaborations, please open an issue in the repository.

---

## üôè Acknowledgments

- **University of San Diego** - Academic support and resources
- **Shiley Marcos School of Engineering** - Research facilities
- **Open-source Community** - scikit-learn, PyMC, and ecosystem libraries
- **LLM Developers** - OpenAI, Anthropic, Meta for advancing NLP capabilities
- **Critical Pedagogy Scholars** - Theoretical foundations on educational bias

---

**Last Updated:** November 2025
**Version:** 1.0
**Status:** Complete Research Framework
